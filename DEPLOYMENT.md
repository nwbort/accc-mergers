# Deployment configuration

This document describes the deployment setup for the ACCC Merger Tracker using Cloudflare Pages.

## Overview

The architecture is fully static:

- **GitHub Actions** scrapes ACCC website and generates all data files
- **Cloudflare Pages** serves the React frontend and static JSON data
- No backend server required
```
GitHub Actions (hourly scrape, daily extract)
    ↓
Scrapes ACCC → matters/*.html
    ↓
Extracts → mergers.json
    ↓
Generates static data files:
  - merger-tracker/frontend/public/data/mergers.json
  - merger-tracker/frontend/public/data/stats.json
  - merger-tracker/frontend/public/data/timeline.json
  - merger-tracker/frontend/public/data/industries.json
  - merger-tracker/frontend/public/data/upcoming-events.json
    ↓
Commits to main branch
    ↓
Cloudflare Pages auto-deploys
```

## Cloudflare Pages configuration

### Build settings

- **Framework preset**: None (or Vite)
- **Build command**: `npm run build && cp -r ../../matters dist/matters`
- **Build output directory**: `merger-tracker/frontend/dist`
- **Root directory**: `merger-tracker/frontend`

### Custom domain

Configure your custom domain (e.g., `mergers.fyi`) in Cloudflare Pages settings.

## GitHub workflows

### `.github/workflows/scrape.yml`

Runs hourly to scrape the ACCC website for new merger pages.

### `.github/workflows/extract.yml`

Runs daily (and on changes to `matters/`) to:
1. Extract merger data from HTML files → `mergers.json`
2. Generate static data files → `merger-tracker/frontend/public/data/*.json`
3. Commit changes to trigger Cloudflare Pages deployment

### `.github/workflows/convert.yml`

Runs after extract workflow to convert any DOCX attachments to PDF using LibreOffice.

## Static data files

All data files are pre-generated by `generate_static_data.py`:

| File | Description |
|------|-------------|
| `mergers.json` | All mergers with full details |
| `stats.json` | Aggregated statistics (counts, averages, medians) |
| `timeline.json` | All events sorted by date |
| `industries.json` | ANZSIC codes with merger counts |
| `upcoming-events.json` | Future consultation/determination dates |

### Regenerating data locally
```bash
python generate_static_data.py
```

This reads `mergers.json` and outputs to `merger-tracker/frontend/public/data/`.

## Local development
```bash
cd merger-tracker/frontend
npm install
npm run dev
```

The dev server serves static JSON from `public/data/`.

### Full pipeline (optional)
```bash
# 1. Scrape (or use existing matters/ data)
./scrape.sh

# 2. Extract merger data
python extract_mergers.py

# 3. Generate static files
python generate_static_data.py

# 4. Run frontend
cd merger-tracker/frontend
npm run dev
```

## Business day calculations

Business day calculations happen client-side using:
- `merger-tracker/frontend/src/utils/dates.js`
- `merger-tracker/frontend/src/data/act-public-holidays.json`

The static data includes raw dates; the frontend calculates business days at render time.

## Benefits

- **$0/month** hosting (Cloudflare Pages free tier)
- **Global CDN** with fast load times
- **No server maintenance**
- **Version controlled data** with full git history
- **Simple deployment** — just push to main

## Limitations

- **No real-time updates** — data refreshes on GitHub Actions schedule
- **No user-generated content** — all data is public/read-only

## Monitoring

1. **GitHub Actions**: Check workflow runs for scrape/extract success
2. **Cloudflare Pages**: Check deployment status in dashboard
3. **Data freshness**: Compare `mergers.json` timestamps with ACCC website

## Troubleshooting

### Data not updating

1. Check GitHub Actions workflow completed successfully
2. Verify `mergers.json` was updated (check git history)
3. Verify static data files were regenerated
4. Check Cloudflare Pages deployment succeeded

### Build failures

1. Check Node.js version matches `.nvmrc`
2. Run `npm install` locally to verify dependencies
3. Check for errors in build output
